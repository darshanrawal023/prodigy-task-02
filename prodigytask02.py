# -*- coding: utf-8 -*-
"""Prodigytask02.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UZkXOBrSiEA7aiRpq8ibithky8rsdKzx
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from IPython.display import Image, display
# %matplotlib inline

#load the train dataset
train = pd.read_csv('/content/train.csv')

#inspect the first few rows of the train dataset
display(train.head())

# set the index to passengerId
train = train.set_index('PassengerId')

#load the test dataset
test = pd.read_csv('/content/test (1).csv')

#inspect the first few rows of the test dataset
display(test.head())

#by calling the shape attribute of the train dataset we can observe that there are 891 observations and 11 columns
#in the data set
train.shape

# Check out the data summary
# Age, Cabin and Embarked has missing data
train.head()

#identify datatypes of the 11 columns, add the stats to the datadict
datadict = pd.DataFrame(train.dtypes)
datadict

# identify missing values of the 11 columns,add the stats to the datadict
datadict['MissingVal'] = train.isnull().sum()
datadict

# Identify number of unique values, For object nunique will the number of levels
# Add the stats the data dict
datadict['NUnique']=train.nunique()
datadict

# Identify the count for each variable, add the stats to datadict
datadict['Count']=train.count()
datadict

# rename the 0 column
datadict = datadict.rename(columns={0:'DataType'})
datadict

# get discripte statistcs on "object" datatypes
train.describe(include=['object'])

# get discriptive statistcs on "number" datatypes
train.describe(include=['number'])

train.Survived.value_counts(normalize=True)

import seaborn as sns
import matplotlib.pyplot as plt

fig, axes = plt.subplots(2, 4, figsize=(19, 15))

# The original code passed 'data=train' which is redundant when using an axes object.
# We need to specify 'x' for the column to plot in the countplot.
sns.countplot(x='Survived', data=train, ax=axes[0, 0])
sns.countplot(x='Pclass', data=train, ax=axes[1, 1])

# Corrected the index for axes to be within the bounds (0-1 for rows, 0-3 for columns)
sns.countplot(x='Sex', data=train, ax=axes[0, 2])  # Changed from axes[3, 2] to axes[0, 2]
sns.countplot(x='SibSp', data=train, ax=axes[1, 3])

# This plot was overlapping with Pclass, moved to a different subplot
sns.countplot(x='Parch', data=train, ax=axes[0, 1])  # Changed from axes[1, 1] to axes[0, 1]

sns.countplot(x='Embarked', data=train, ax=axes[1, 3]) # This subplot now have both 'Sibsp' and 'Embarked'

# Adjusted the indices to fit within the 2x4 grid
sns.histplot(train['Fare'], kde=True, ax=axes[0, 3])  # Changed from axes[1, 3] to axes[0, 3]
sns.histplot(train['Age'].dropna(), kde=True, ax=axes[1, 0]) # Changed from axes[1, 5] to axes[1, 0]

plt.show()

figbi, axesbi = plt.subplots(3, 6, figsize=(20, 13))
train.groupby('Pclass')['Survived'].mean().plot(kind='barh',ax=axesbi[1,2],xlim=[1,1])
train.groupby('SibSp')['Survived'].mean().plot(kind='barh',ax=axesbi[1,1],xlim=[1,2])
train.groupby('Parch')['Survived'].mean().plot(kind='barh',ax=axesbi[2,2],xlim=[1,3])
train.groupby('Sex')['Survived'].mean().plot(kind='barh',ax=axesbi[2,3],xlim=[1,3])
train.groupby('Embarked')['Survived'].mean().plot(kind='barh',ax=axesbi[1,2],xlim=[0,1])
sns.boxplot(x="Survived", y="Age", data=train,ax=axesbi[1,1])
sns.boxplot(x="Survived", y="Fare", data=train,ax=axesbi[1,3])

sns.jointplot(x="Age", y="Fare", data=train);

import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

f, ax = plt.subplots(figsize=(10, 8))

# Select only numerical features for correlation calculation
numerical_features = train.select_dtypes(include=np.number)
corr = numerical_features.corr()

sns.heatmap(
    corr,
    mask=np.zeros_like(corr, dtype=bool),  # Changed `np.bool` to `bool`
    cmap=sns.diverging_palette(220, 10, as_cmap=True),
    square=True,
    ax=ax
)

plt.show()

train['Name_len']=train.Name.str.len()

train['Ticket_First']=train.Ticket.str[0]

train['FamilyCount']=train.SibSp+train.Parch

train['Cabin_First']=train.Cabin.str[0]

# Regular expression to get the title of the Name
train['title'] = train.Name.str.extract('\, ([A-Z][^ ]*\.)',expand=False)

train.title.value_counts().reset_index()

# we see that there are 15 Zero values and its reasonbale
# to flag them as missing values since every ticket
# should have a value greater than 0
print((train.Fare == 0).sum())

#mark zero values as missing or NaN
train.Fare = train.Fare.replace(0, np.NaN)

# validate to see if there are no more zero values
print((train.Fare == 0).sum())

# keep the index
train[train.Fare.isnull()].index

train.Fare.mean()

# impute the missing Fare values with the mean Fare value
train.Fare.fillna(train.Fare.mean(),inplace=True)

# validate if any null values are present after the imputation
train[train.Fare.isnull()]

#we see that there are 0 Zero values
print((train.Age == 0).sum())

# impute the missing Age values with the mean Fare value
train.Age.fillna(train.Age.mean(),inplace=True)

# validate if any null values are present after the imputation
train[train.Age.isnull()]

# We see that a majority 77% of the Cabin variable has missing values.
# Hence will drop the column from training a machine learnign algorithem
train.Cabin.isnull().mean()

train.info()

train.columns

trainML = train[['Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket',
       'Fare', 'Embarked', 'Name_len', 'Ticket_First', 'FamilyCount',
       'title']]

# drop rows of missing values
trainML = trainML.dropna()

# check the datafram has any missing values
trainML.isnull().sum()

# Import Estimator AND Instantiate estimator class to create an estimator object
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()

X_Age = trainML[['Age']].values
y = trainML['Survived'].values
# Use the fit method to train
lr.fit(X_Age,y)
# Make a prediction
y_predict = lr.predict(X_Age)
y_predict[:10]
(y == y_predict).mean()

X_Fare = trainML[['Fare']].values
y = trainML['Survived'].values
# Use the fit method to train
lr.fit(X_Fare,y)
# Make a prediction
y_predict = lr.predict(X_Fare)
y_predict[:10]
(y == y_predict).mean()

X_sex = pd.get_dummies(trainML['Sex']).values
y = trainML['Survived'].values
# Use the fit method to train
lr.fit(X_sex, y)
# Make a prediction
y_predict = lr.predict(X_sex)
y_predict[:10]
(y == y_predict).mean()

X_pclass = pd.get_dummies(trainML['Pclass']).values
y = trainML['Survived'].values
lr = LogisticRegression()
lr.fit(X_pclass, y)
# Make a prediction
y_predict = lr.predict(X_pclass)
y_predict[:10]
(y == y_predict).mean()

from sklearn.ensemble import RandomForestClassifier
X=trainML[['Age', 'SibSp', 'Parch',
       'Fare', 'Name_len', 'FamilyCount']].values # Taking all the numerical values
y = trainML['Survived'].values
RF = RandomForestClassifier()
RF.fit(X, y)
# Make a prediction
y_predict = RF.predict(X)
y_predict[:10]
(y == y_predict).mean()